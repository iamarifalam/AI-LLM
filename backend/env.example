PORT=5000
NODE_ENV=development

# Ollama Configuration
# Set USE_OLLAMA=true to enable Ollama integration
USE_OLLAMA=true

# Ollama API URL (default: http://localhost:11434 for local Ollama)
# For Ollama Cloud, use your cloud endpoint URL
OLLAMA_API_URL=http://localhost:11434

# Ollama API Key (required for Ollama Cloud, optional for local)
# Put your Ollama API key here
OLLAMA_API_KEY=your_ollama_api_key_here

# Ollama Model name (e.g., llama2, mistral, codellama, llama3, etc.)
# Recommended models for better responses: llama3, mistral, llama3.1, qwen2.5
OLLAMA_MODEL=llama2

# Optional: Model parameters for better response quality
# OLLAMA_TEMPERATURE=0.7        # 0.0-1.0, lower = more focused, higher = more creative
# OLLAMA_TOP_P=0.9              # 0.0-1.0, controls diversity
# OLLAMA_TOP_K=40               # Limits vocabulary to top K tokens
# OLLAMA_NUM_PREDICT=2048       # Maximum tokens to generate (longer responses)
# OLLAMA_REPEAT_PENALTY=1.1     # Reduces repetition (1.0 = no penalty)

